//! Algorithm Hypothesis Test - ç•°ãªã‚‹åœ§ç¸®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä»®èª¬ã®æ¤œè¨¼
//! å¹³å‡ãƒãƒƒãƒé•·149.5ãƒã‚¤ãƒˆã®ç•°å¸¸æ€§ã‹ã‚‰ã€ç‰¹æ®ŠæŠ€è¡“ã®å­˜åœ¨ã‚’èª¿æŸ»

use anyhow::Result;
use std::collections::HashMap;

fn main() -> Result<()> {
    println!("ğŸ§¬ Algorithm Hypothesis Test - Special Compression Techniques");
    println!("==========================================================");
    println!("ğŸ¯ Investigating: Why average match length is 149.5 bytes");
    println!("ğŸ” Hypothesis: Non-standard LZSS or hybrid algorithm");
    println!();
    
    let test_file = "test_assets/lf2/C0101.LF2";
    investigate_special_algorithm(test_file)?;
    
    Ok(())
}

fn investigate_special_algorithm(test_file: &str) -> Result<()> {
    use retro_decode::formats::toheart::lf2::Lf2Image;
    
    let original_bytes = std::fs::read(test_file)?;
    let original_image = Lf2Image::open(test_file)?;
    let pixels = &original_image.pixels;
    
    let header_size = 8 + 8 + 1 + 1 + (original_image.color_count as usize * 3);
    let compressed_data = &original_bytes[header_size..];
    
    println!("ğŸ“Š ALGORITHM INVESTIGATION");
    println!("=========================");
    
    // ä»®èª¬1: ç‰¹æ®Šãªãƒãƒƒãƒé•·ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    investigate_match_length_encoding(compressed_data)?;
    
    // ä»®èª¬2: è¾æ›¸ãƒ™ãƒ¼ã‚¹åœ§ç¸®
    investigate_dictionary_compression(compressed_data, pixels)?;
    
    // ä»®èª¬3: RLE + LZSS ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
    investigate_rle_hybrid(compressed_data, pixels)?;
    
    // ä»®èª¬4: ç‰¹æ®Šãªå‰å‡¦ç†
    investigate_preprocessing(pixels)?;
    
    // ä»®èª¬5: åˆ†å‰²åœ§ç¸®
    investigate_block_compression(compressed_data, pixels)?;
    
    Ok(())
}

fn investigate_match_length_encoding(data: &[u8]) -> Result<()> {
    println!("ğŸ”¬ HYPOTHESIS 1: Special Match Length Encoding");
    println!("===============================================");
    
    let mut pos = 0;
    let mut length_frequencies = HashMap::new();
    let mut ultra_long_matches = Vec::new();
    let mut suspicious_patterns = Vec::new();
    
    while pos < data.len() {
        let byte = data[pos];
        
        if byte & 0x80 != 0 {
            if pos + 2 < data.len() {
                let distance = (((byte & 0x0F) as usize) << 8) | (data[pos + 1] as usize);
                let length = data[pos + 2] as usize;
                
                *length_frequencies.entry(length).or_insert(0) += 1;
                
                // ç•°å¸¸ã«é•·ã„ãƒãƒƒãƒ
                if length > 100 {
                    ultra_long_matches.push((pos, distance, length));
                }
                
                // ç–‘ã‚ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³
                if length == 255 || distance == 0 || length == 0 {
                    suspicious_patterns.push((pos, distance, length, byte, data[pos + 1], data[pos + 2]));
                }
                
                pos += 3;
            } else {
                pos += 1;
            }
        } else {
            pos += 1;
        }
    }
    
    // é•·ã•ã®åˆ†å¸ƒè§£æ
    let mut sorted_lengths: Vec<_> = length_frequencies.iter().collect();
    sorted_lengths.sort_by_key(|(len, _)| **len);
    
    println!("   ğŸ“Š Length Distribution Analysis:");
    println!("      Most common lengths:");
    for (i, (&length, &count)) in sorted_lengths.iter().rev().take(10).enumerate() {
        let percentage = count as f64 / sorted_lengths.len() as f64 * 100.0;
        println!("         {}. Length {}: {} times ({:.1}%)", i + 1, length, count, percentage);
    }
    
    // ç‰¹æ®Šé•·ã•ã®æ¤œå‡º
    let length_255_count = length_frequencies.get(&255).unwrap_or(&0);
    let length_0_count = length_frequencies.get(&0).unwrap_or(&0);
    
    println!("   ğŸ” Special Length Analysis:");
    println!("      Length 255 (max): {} occurrences", length_255_count);
    println!("      Length 0 (invalid): {} occurrences", length_0_count);
    println!("      Ultra-long matches (>100): {} found", ultra_long_matches.len());
    
    if ultra_long_matches.len() > 10 {
        println!("      ğŸš¨ ANOMALY: Excessive ultra-long matches suggests special encoding!");
    }
    
    // ç–‘ã‚ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³
    println!("   ğŸ” Suspicious Patterns:");
    for (i, (pos, dist, len, b0, b1, b2)) in suspicious_patterns.iter().take(10).enumerate() {
        println!("      {}. Pos {}: dist={}, len={} [bytes: {:02X} {:02X} {:02X}]", 
                i + 1, pos, dist, len, b0, b1, b2);
    }
    
    println!();
    Ok(())
}

fn investigate_dictionary_compression(compressed: &[u8], pixels: &[u8]) -> Result<()> {
    println!("ğŸ”¬ HYPOTHESIS 2: Dictionary-Based Compression");
    println!("==============================================");
    
    // è¾æ›¸ã‚‰ã—ããƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
    let mut frequent_sequences = HashMap::new();
    
    // 4-16ãƒã‚¤ãƒˆã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ¤œç´¢
    for seq_len in 4..=16 {
        for i in 0..pixels.len().saturating_sub(seq_len) {
            let sequence = &pixels[i..i + seq_len];
            *frequent_sequences.entry(sequence.to_vec()).or_insert(0) += 1;
        }
    }
    
    // é »å‡ºã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®è§£æ
    let mut sorted_seqs: Vec<_> = frequent_sequences.iter()
        .filter(|(_, &count)| count >= 10)  // 10å›ä»¥ä¸Šå‡ºç¾
        .collect();
    sorted_seqs.sort_by_key(|(_, &count)| std::cmp::Reverse(count));
    
    println!("   ğŸ“Š Frequent Pixel Sequences (â‰¥10 occurrences):");
    for (i, (seq, &count)) in sorted_seqs.iter().take(10).enumerate() {
        let hex_seq: Vec<String> = seq.iter().take(8).map(|b| format!("{:02X}", b)).collect();
        println!("      {}. [{}{}]: {} times, {} bytes",
                i + 1, 
                hex_seq.join(" "),
                if seq.len() > 8 { "..." } else { "" },
                count,
                seq.len());
    }
    
    // è¾æ›¸åŠ¹ç‡ã®è¨ˆç®—
    let total_sequence_bytes: usize = sorted_seqs.iter()
        .map(|(seq, &count)| seq.len() * count)
        .sum();
    
    let potential_savings = total_sequence_bytes.saturating_sub(sorted_seqs.len() * 3); // 3ãƒã‚¤ãƒˆ = ãƒãƒƒãƒã‚³ã‚¹ãƒˆ
    let compression_potential = potential_savings as f64 / pixels.len() as f64 * 100.0;
    
    println!("   ğŸ“Š Dictionary Compression Potential:");
    println!("      Total frequent sequence bytes: {}", total_sequence_bytes);
    println!("      Potential dictionary size: {} entries", sorted_seqs.len());
    println!("      Estimated savings: {} bytes ({:.1}%)", potential_savings, compression_potential);
    
    if compression_potential > 30.0 {
        println!("      ğŸ”¥ HIGH POTENTIAL: Dictionary compression could explain efficiency!");
    }
    
    println!();
    Ok(())
}

fn investigate_rle_hybrid(compressed: &[u8], pixels: &[u8]) -> Result<()> {
    println!("ğŸ”¬ HYPOTHESIS 3: RLE + LZSS Hybrid");
    println!("====================================");
    
    // ãƒ”ã‚¯ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ã®RLEè§£æ
    let mut rle_runs = Vec::new();
    let mut current_value = pixels[0];
    let mut current_run = 1;
    
    for &pixel in &pixels[1..] {
        if pixel == current_value {
            current_run += 1;
        } else {
            rle_runs.push((current_value, current_run));
            current_value = pixel;
            current_run = 1;
        }
    }
    rle_runs.push((current_value, current_run));
    
    // RLEåŠ¹ç‡ã®è¨ˆç®—
    let long_runs: Vec<_> = rle_runs.iter()
        .filter(|(_, len)| *len >= 4)
        .collect();
    
    let total_run_pixels: usize = long_runs.iter().map(|(_, len)| *len).sum();
    let rle_savings = total_run_pixels.saturating_sub(long_runs.len() * 2); // 2ãƒã‚¤ãƒˆ = RLEã‚³ã‚¹ãƒˆ
    let rle_efficiency = rle_savings as f64 / pixels.len() as f64 * 100.0;
    
    println!("   ğŸ“Š RLE Analysis:");
    println!("      Total runs: {}", rle_runs.len());
    println!("      Long runs (â‰¥4): {}", long_runs.len());
    println!("      Long run pixels: {}", total_run_pixels);
    println!("      RLE potential savings: {} bytes ({:.1}%)", rle_savings, rle_efficiency);
    
    // æœ€é•·ãƒ©ãƒ³ã®æ¤œå‡º
    if let Some((value, max_len)) = long_runs.iter().max_by_key(|(_, len)| *len) {
        println!("      Longest run: {} pixels of value 0x{:02X}", max_len, value);
    }
    
    // ãƒãƒƒãƒã®é•·ã•ã¨RLEã®ç›¸é–¢
    let mut match_vs_rle_correlation = 0;
    let mut pos = 0;
    let mut match_lengths = Vec::new();
    
    while pos < compressed.len() {
        let byte = compressed[pos];
        if byte & 0x80 != 0 && pos + 2 < compressed.len() {
            let length = compressed[pos + 2] as usize;
            match_lengths.push(length);
            pos += 3;
        } else {
            pos += 1;
        }
    }
    
    // é•·ã„ãƒãƒƒãƒãŒRLEãƒ©ãƒ³ã¨ç›¸é–¢ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    for &match_len in &match_lengths {
        if match_len >= 50 {  // é•·ã„ãƒãƒƒãƒ
            for (_, run_len) in &long_runs {
                if (match_len as i32 - *run_len as i32).abs() <= 5 {  // è¿‘ä¼¼ä¸€è‡´
                    match_vs_rle_correlation += 1;
                    break;
                }
            }
        }
    }
    
    println!("   ğŸ” Match-RLE Correlation:");
    println!("      Long matches matching RLE runs: {}", match_vs_rle_correlation);
    
    if match_vs_rle_correlation > 10 {
        println!("      ğŸ”¥ CORRELATION DETECTED: RLE preprocessing likely!");
    }
    
    println!();
    Ok(())
}

fn investigate_preprocessing(pixels: &[u8]) -> Result<()> {
    println!("ğŸ”¬ HYPOTHESIS 4: Special Preprocessing");
    println!("=======================================");
    
    // ç”»åƒã®ç‰¹æ€§è§£æ
    let mut pixel_histogram = [0usize; 256];
    for &pixel in pixels {
        pixel_histogram[pixel as usize] += 1;
    }
    
    // è‰²åˆ†å¸ƒã®åˆ†æ
    let unique_colors = pixel_histogram.iter().filter(|&&count| count > 0).count();
    let most_common_color = pixel_histogram.iter()
        .enumerate()
        .max_by_key(|(_, &count)| count)
        .map(|(color, &count)| (color, count))
        .unwrap_or((0, 0));
    
    println!("   ğŸ“Š Pixel Distribution Analysis:");
    println!("      Unique colors: {} / 256", unique_colors);
    println!("      Most common color: 0x{:02X} ({} pixels, {:.1}%)", 
             most_common_color.0, most_common_color.1,
             most_common_color.1 as f64 / pixels.len() as f64 * 100.0);
    
    // ãƒ‘ãƒ¬ãƒƒãƒˆæœ€é©åŒ–ã®å¯èƒ½æ€§
    let palette_efficiency = unique_colors as f64 / 256.0;
    println!("      Palette efficiency: {:.1}%", palette_efficiency * 100.0);
    
    if palette_efficiency < 0.5 {
        println!("      ğŸ”¥ LOW PALETTE USAGE: Custom palette ordering likely!");
    }
    
    // ç©ºé–“çš„å±€æ‰€æ€§ã®è§£æ
    let width = 246; // ç”»åƒå¹…
    let mut spatial_correlation = 0;
    
    for y in 0..428 - 1 {  // ç”»åƒé«˜ã•-1
        for x in 0..width - 1 {
            let current_idx = y * width + x;
            let right_idx = current_idx + 1;
            let down_idx = (y + 1) * width + x;
            
            if current_idx < pixels.len() && right_idx < pixels.len() && down_idx < pixels.len() {
                if pixels[current_idx] == pixels[right_idx] || 
                   pixels[current_idx] == pixels[down_idx] {
                    spatial_correlation += 1;
                }
            }
        }
    }
    
    let total_comparisons = (428 - 1) * (width - 1) * 2;
    let spatial_coherence = spatial_correlation as f64 / total_comparisons as f64 * 100.0;
    
    println!("   ğŸ“Š Spatial Coherence Analysis:");
    println!("      Adjacent pixel similarity: {:.1}%", spatial_coherence);
    
    if spatial_coherence > 70.0 {
        println!("      ğŸ”¥ HIGH SPATIAL COHERENCE: Image preprocessing likely!");
    }
    
    println!();
    Ok(())
}

fn investigate_block_compression(compressed: &[u8], pixels: &[u8]) -> Result<()> {
    println!("ğŸ”¬ HYPOTHESIS 5: Block-Based Compression");
    println!("=========================================");
    
    // åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‚’1KBãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã—ã¦è§£æ
    let block_size = 1024;
    let blocks: Vec<_> = compressed.chunks(block_size).collect();
    
    println!("   ğŸ“Š Block Analysis ({} byte blocks):", block_size);
    println!("      Total blocks: {}", blocks.len());
    
    let mut block_entropies = Vec::new();
    let mut block_match_ratios = Vec::new();
    
    for (i, block) in blocks.iter().enumerate() {
        // ãƒ–ãƒ­ãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        let mut byte_counts = [0usize; 256];
        for &byte in block.iter() {
            byte_counts[byte as usize] += 1;
        }
        
        let entropy = byte_counts.iter()
            .filter(|&&count| count > 0)
            .map(|&count| {
                let p = count as f64 / block.len() as f64;
                -p * p.log2()
            })
            .sum::<f64>();
        
        block_entropies.push(entropy);
        
        // ãƒãƒƒãƒæ¯”ç‡
        let match_bytes = block.iter().filter(|&&b| b >= 0x80).count();
        let match_ratio = match_bytes as f64 / block.len() as f64;
        block_match_ratios.push(match_ratio);
        
        if i < 5 {  // æœ€åˆã®5ãƒ–ãƒ­ãƒƒã‚¯
            println!("      Block {}: entropy={:.3}, match_ratio={:.1}%", 
                     i + 1, entropy, match_ratio * 100.0);
        }
    }
    
    // ãƒ–ãƒ­ãƒƒã‚¯é–“ã®å¤‰å‹•è§£æ
    if !block_entropies.is_empty() {
        let avg_entropy = block_entropies.iter().sum::<f64>() / block_entropies.len() as f64;
        let entropy_variance = block_entropies.iter()
            .map(|e| (e - avg_entropy).powi(2))
            .sum::<f64>() / block_entropies.len() as f64;
        
        let avg_match_ratio = block_match_ratios.iter().sum::<f64>() / block_match_ratios.len() as f64;
        let match_ratio_variance = block_match_ratios.iter()
            .map(|r| (r - avg_match_ratio).powi(2))
            .sum::<f64>() / block_match_ratios.len() as f64;
        
        println!("   ğŸ“Š Block Variation Analysis:");
        println!("      Entropy: avg={:.3}, variance={:.3}", avg_entropy, entropy_variance);
        println!("      Match ratio: avg={:.1}%, variance={:.3}", avg_match_ratio * 100.0, match_ratio_variance);
        
        if entropy_variance < 0.1 && match_ratio_variance < 0.01 {
            println!("      ğŸ”¥ UNIFORM BLOCKS: Consistent compression suggests block preprocessing!");
        } else if entropy_variance > 1.0 {
            println!("      ğŸ”¥ VARIABLE BLOCKS: Adaptive compression per block likely!");
        }
    }
    
    // ãƒ”ã‚¯ã‚»ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã®åˆ†æ
    let pixel_block_size = pixels.len() / blocks.len();
    println!("   ğŸ“Š Pixel-to-Compression Ratio:");
    println!("      Avg pixels per compression block: {}", pixel_block_size);
    println!("      Compression ratio per block: {:.3}", block_size as f64 / pixel_block_size as f64);
    
    println!();
    Ok(())
}