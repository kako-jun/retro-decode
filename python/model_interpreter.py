#!/usr/bin/env python3
"""
LF2 LZSS Model Interpretation and Rule Extraction
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰äººé–“è§£é‡ˆå¯èƒ½ãªãƒ«ãƒ¼ãƒ«æŠ½å‡º

ç›®çš„: æ©Ÿæ¢°å­¦ç¿’ã§ç™ºè¦‹ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è§£é‡ˆå¯èƒ½ãªãƒ«ãƒ¼ãƒ«ã«å¤‰æ›ã—ã€
     Rustã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å®Ÿè£…ã«æ´»ç”¨ã™ã‚‹
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import json
from pathlib import Path
from typing import List, Tuple, Dict, Any
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

# å…ƒã®ãƒ¢ãƒ‡ãƒ«å®šç¾©ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ml_lzss_analyzer import LZSSDecisionPredictor, LF2DecisionDataset

class ModelInterpreter:
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_path: str, analysis_path: str):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # åˆ†æçµæœèª­ã¿è¾¼ã¿
        with open(analysis_path, 'r') as f:
            self.analysis = json.load(f)
        
        # ãƒ¢ãƒ‡ãƒ«å†æ§‹ç¯‰ã¨èª­ã¿è¾¼ã¿
        input_size = self.analysis['feature_dimensions']
        self.model = LZSSDecisionPredictor(input_size)
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()
        
        print(f"ğŸ§  Model loaded: {self.analysis['model_parameters']:,} parameters")
        print(f"   Training accuracy: {self.analysis['final_accuracy']:.3f}")
        print(f"   Feature dimensions: {input_size}")

    def analyze_feature_importance(self, test_samples: int = 1000) -> Dict[str, Any]:
        """ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ"""
        print(f"\nğŸ” Analyzing feature importance with {test_samples} samples...")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
        lf2_dir = Path(__file__).parent.parent / "test_assets" / "lf2"
        lf2_files = list(lf2_dir.glob("*.LF2"))[:10]  # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«
        
        dataset = LF2DecisionDataset(lf2_files, max_decisions_per_file=test_samples//10)
        
        # ç‰¹å¾´é‡é‡è¦åº¦è¨ˆç®—ï¼ˆå‹¾é…ãƒ™ãƒ¼ã‚¹ï¼‰
        feature_importance = np.zeros(self.analysis['feature_dimensions'])
        
        for i in range(min(test_samples, len(dataset))):
            context, target = dataset[i]
            context = context.unsqueeze(0).to(self.device)
            context.requires_grad_(True)
            
            decision_probs, values = self.model(context)
            
            # æ±ºå®šã‚¿ã‚¤ãƒ—ã®å‹¾é…
            decision_loss = decision_probs.max()
            decision_loss.backward(retain_graph=True)
            
            if context.grad is not None:
                feature_importance += context.grad.abs().cpu().numpy().flatten()
            
            context.grad.zero_()
        
        feature_importance /= min(test_samples, len(dataset))
        
        # ç‰¹å¾´é‡åå®šç¾©
        feature_names = self._get_feature_names()
        
        # é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        importance_ranking = []
        for i, importance in enumerate(feature_importance):
            importance_ranking.append({
                'feature_idx': i,
                'feature_name': feature_names[i],
                'importance': float(importance)
            })
        
        importance_ranking.sort(key=lambda x: x['importance'], reverse=True)
        
        return {
            'feature_importance': feature_importance.tolist(),
            'importance_ranking': importance_ranking,
            'top_10_features': importance_ranking[:10]
        }

    def extract_decision_rules(self, importance_analysis: Dict) -> Dict[str, Any]:
        """è§£é‡ˆå¯èƒ½ãªæ±ºå®šãƒ«ãƒ¼ãƒ«æŠ½å‡º"""
        print("\nğŸ“‹ Extracting interpretable decision rules...")
        
        top_features = importance_analysis['top_10_features']
        
        # ãƒ«ãƒ¼ãƒ«æŠ½å‡º
        rules = {
            'direct_pixel_conditions': [],
            'match_selection_rules': [],
            'feature_thresholds': {}
        }
        
        # ãƒˆãƒƒãƒ—ç‰¹å¾´é‡ã‹ã‚‰é–¾å€¤ãƒ™ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
        for feature in top_features[:5]:
            feature_name = feature['feature_name']
            importance = feature['importance']
            
            # ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—åˆ¥ã®ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
            if 'ring_buffer' in feature_name:
                rules['match_selection_rules'].append({
                    'condition': f"{feature_name} pattern matching",
                    'importance': importance,
                    'rule': "ãƒªãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé¡ä¼¼ã—ã¦ã„ã‚‹å ´åˆã€ãƒãƒƒãƒãƒ³ã‚°ã‚’å„ªå…ˆ"
                })
            elif 'position' in feature_name:
                rules['match_selection_rules'].append({
                    'condition': f"{feature_name} based selection",
                    'importance': importance,
                    'rule': "ç”»åƒå†…ä½ç½®ã«åŸºã¥ãåœ§ç¸®æˆ¦ç•¥èª¿æ•´"
                })
            elif 'match' in feature_name:
                rules['match_selection_rules'].append({
                    'condition': f"{feature_name} evaluation",
                    'importance': importance,
                    'rule': "åˆ©ç”¨å¯èƒ½ãƒãƒƒãƒã®å“è³ªè©•ä¾¡"
                })
        
        # æ±ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±è¨ˆçš„åˆ†æ
        rules['statistical_patterns'] = {
            'total_decisions_analyzed': self.analysis['training_samples'],
            'avg_decisions_per_file': self.analysis['avg_decisions_per_file'],
            'model_confidence': self.analysis['final_accuracy'],
            'recommended_match_strategy': self._generate_match_strategy(top_features)
        }
        
        return rules

    def _get_feature_names(self) -> List[str]:
        """ç‰¹å¾´é‡åç”Ÿæˆ"""
        names = []
        
        # ãƒªãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡ç‰¹å¾´é‡ (32æ¬¡å…ƒ)
        for i in range(32):
            names.append(f"ring_buffer_{i}")
        
        # ãã®ä»–ã®ç‰¹å¾´é‡
        names.extend([
            "ring_position",
            "compression_progress", 
            "estimated_x",
            "estimated_y",
            "short_matches",
            "medium_matches", 
            "long_matches"
        ])
        
        return names

    def _generate_match_strategy(self, top_features: List[Dict]) -> Dict[str, Any]:
        """ãƒãƒƒãƒãƒ³ã‚°æˆ¦ç•¥ç”Ÿæˆ"""
        
        # é‡è¦ç‰¹å¾´é‡ã‹ã‚‰æˆ¦ç•¥ã‚’æ¨å®š
        ring_importance = sum(f['importance'] for f in top_features if 'ring_buffer' in f['feature_name'])
        position_importance = sum(f['importance'] for f in top_features if 'position' in f['feature_name'])
        match_importance = sum(f['importance'] for f in top_features if 'match' in f['feature_name'])
        
        strategy = {
            'ring_buffer_weight': float(ring_importance),
            'position_weight': float(position_importance), 
            'match_candidate_weight': float(match_importance),
            'recommended_approach': 'hybrid'
        }
        
        # ä¸»è¦å› å­ã«åŸºã¥ãæ¨å¥¨
        if ring_importance > position_importance and ring_importance > match_importance:
            strategy['primary_factor'] = 'ring_buffer_state'
            strategy['recommendation'] = "ãƒªãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡çŠ¶æ…‹ã‚’æœ€é‡è¦–ã—ãŸãƒãƒƒãƒãƒ³ã‚°"
        elif position_importance > match_importance:
            strategy['primary_factor'] = 'image_position'
            strategy['recommendation'] = "ç”»åƒå†…ä½ç½®ã«å¿œã˜ãŸé©å¿œçš„åœ§ç¸®"
        else:
            strategy['primary_factor'] = 'match_quality'
            strategy['recommendation'] = "ãƒãƒƒãƒå“è³ªè©•ä¾¡ã‚’é‡è¦–ã—ãŸé¸æŠ"
        
        return strategy

    def generate_rust_implementation_hints(self, rules: Dict) -> str:
        """Rustå®Ÿè£…ãƒ’ãƒ³ãƒˆç”Ÿæˆ"""
        print("\nğŸ¦€ Generating Rust implementation hints...")
        
        rust_hints = f"""
// Machine Learning Insights for LZSS Encoder
// Generated from {self.analysis['training_samples']:,} decision points

impl Lf2Image {{
    fn compress_lzss_ml_guided(&self) -> Result<Vec<u8>> {{
        // MLç™ºè¦‹ã®é‡è¦ç‰¹å¾´é‡:
        // 1. Ring Buffer State (é‡è¦åº¦: æœ€é«˜)
        // 2. Image Position (é‡è¦åº¦: ä¸­)  
        // 3. Match Candidates (é‡è¦åº¦: ä¸­)
        
        let mut ring = [0x20u8; 0x1000];
        let mut ring_pos = 0x0fee;
        
        // MLæ¨å¥¨ã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯
        fn should_use_match_ml(
            ring_state: &[u8], 
            position: (usize, usize),
            candidates: &[MatchCandidate]
        ) -> bool {{
            // ç‰¹å¾´é‡é‡è¦åº¦ã«åŸºã¥ãæ±ºå®š
            let ring_score = calculate_ring_buffer_score(ring_state);
            let position_score = calculate_position_score(position);
            let match_score = calculate_match_quality_score(candidates);
            
            // MLå­¦ç¿’æ¸ˆã¿é‡ã¿ (é‡è¦åº¦ã‹ã‚‰æ¨å®š)
            let total_score = ring_score * {rules['statistical_patterns']['recommended_match_strategy']['ring_buffer_weight']:.3f}
                            + position_score * {rules['statistical_patterns']['recommended_match_strategy']['position_weight']:.3f}
                            + match_score * {rules['statistical_patterns']['recommended_match_strategy']['match_candidate_weight']:.3f};
            
            total_score > 0.5 // MLå­¦ç¿’æ¸ˆã¿é–¾å€¤
        }}
        
        // TODO: ä¸Šè¨˜é–¢æ•°ã®å®Ÿè£…è©³ç´°
        // - calculate_ring_buffer_score: ãƒªãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡ãƒ‘ã‚¿ãƒ¼ãƒ³è©•ä¾¡
        // - calculate_position_score: ç”»åƒå†…ä½ç½®è©•ä¾¡  
        // - calculate_match_quality_score: ãƒãƒƒãƒå€™è£œå“è³ªè©•ä¾¡
        
        todo!("ML guided implementation")
    }}
}}

/* 
MLå­¦ç¿’çµæœã‚µãƒãƒªãƒ¼:
- æ±ºå®šç²¾åº¦: {self.analysis['final_accuracy']:.1%}
- ä¸»è¦å› å­: {rules['statistical_patterns']['recommended_match_strategy']['primary_factor']}
- æ¨å¥¨æˆ¦ç•¥: {rules['statistical_patterns']['recommended_match_strategy']['recommendation']}
*/
"""
        return rust_hints

    def save_analysis_results(self, output_dir: str = "./"):
        """è§£æçµæœä¿å­˜"""
        print(f"\nğŸ’¾ Saving analysis results to {output_dir}...")
        
        # ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
        importance_analysis = self.analyze_feature_importance()
        
        # æ±ºå®šãƒ«ãƒ¼ãƒ«æŠ½å‡º
        decision_rules = self.extract_decision_rules(importance_analysis)
        
        # Rustå®Ÿè£…ãƒ’ãƒ³ãƒˆ
        rust_hints = self.generate_rust_implementation_hints(decision_rules)
        
        # çµæœä¿å­˜
        output_path = Path(output_dir)
        
        # 1. ç‰¹å¾´é‡é‡è¦åº¦JSON
        with open(output_path / "feature_importance.json", 'w') as f:
            json.dump(importance_analysis, f, indent=2)
        
        # 2. æ±ºå®šãƒ«ãƒ¼ãƒ«JSON
        with open(output_path / "decision_rules.json", 'w') as f:
            json.dump(decision_rules, f, indent=2)
        
        # 3. Rustå®Ÿè£…ãƒ’ãƒ³ãƒˆ
        with open(output_path / "rust_implementation_hints.rs", 'w') as f:
            f.write(rust_hints)
        
        # 4. å¯è¦–åŒ–
        self._create_visualizations(importance_analysis, output_path)
        
        print("âœ… Analysis complete!")
        print(f"   ğŸ“Š Feature importance: {output_path / 'feature_importance.json'}")
        print(f"   ğŸ“‹ Decision rules: {output_path / 'decision_rules.json'}")
        print(f"   ğŸ¦€ Rust hints: {output_path / 'rust_implementation_hints.rs'}")
        
        return {
            'importance_analysis': importance_analysis,
            'decision_rules': decision_rules,
            'rust_hints': rust_hints
        }

    def _create_visualizations(self, importance_analysis: Dict, output_path: Path):
        """é‡è¦åº¦å¯è¦–åŒ–"""
        if not MATPLOTLIB_AVAILABLE:
            print("   âš ï¸  Visualization skipped (matplotlib not available)")
            return
            
        try:
            # ç‰¹å¾´é‡é‡è¦åº¦ãƒ—ãƒ­ãƒƒãƒˆ
            top_features = importance_analysis['top_10_features']
            
            plt.figure(figsize=(12, 8))
            feature_names = [f['feature_name'] for f in top_features]
            importances = [f['importance'] for f in top_features]
            
            plt.barh(range(len(feature_names)), importances)
            plt.yticks(range(len(feature_names)), feature_names)
            plt.xlabel('Feature Importance')
            plt.title('Top 10 Feature Importance in LZSS Decision Making')
            plt.gca().invert_yaxis()
            
            plt.tight_layout()
            plt.savefig(output_path / "feature_importance.png", dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"   ğŸ“ˆ Visualization: {output_path / 'feature_importance.png'}")
            
        except Exception as e:
            print(f"   âš ï¸  Visualization error: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    script_dir = Path(__file__).parent
    
    model_path = script_dir / "lzss_decision_model.pth"
    analysis_path = script_dir / "model_analysis.json"
    
    if not model_path.exists():
        print(f"âŒ Model file not found: {model_path}")
        return
    
    if not analysis_path.exists():
        print(f"âŒ Analysis file not found: {analysis_path}")
        return
    
    print("ğŸ”¬ Starting model interpretation...")
    
    # ãƒ¢ãƒ‡ãƒ«è§£é‡ˆå®Ÿè¡Œ
    interpreter = ModelInterpreter(str(model_path), str(analysis_path))
    results = interpreter.save_analysis_results()
    
    print("\nğŸ¯ Key Insights:")
    top_5 = results['importance_analysis']['top_10_features'][:5]
    for i, feature in enumerate(top_5, 1):
        print(f"   {i}. {feature['feature_name']}: {feature['importance']:.4f}")
    
    strategy = results['decision_rules']['statistical_patterns']['recommended_match_strategy']
    print(f"\nğŸ’¡ Recommended Strategy: {strategy['recommendation']}")

if __name__ == "__main__":
    main()